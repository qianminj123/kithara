.. Kithara Documentation master file, created by
   sphinx-quickstart on Wed Nov 20 10:35:12 2024.
   You can adapt this file completely to your liking, but it should at least
   contain the root `toctree` directive.

👋 Welcome to Kithara!
===================================
.. button-link:: https://kithara.readthedocs.io/en/latest/index.html

   https://goo.gle/kithara





.. note::

   This project is under active development.

Introduction
------------

What is Kithara?
~~~~~~~~~~~~~~~~

**Kithara** is a lightweight library offering building blocks and recipes for tuning popular open source LLMs like Llama 3 and Gemma 2 on Google TPUs.

Kithara makes post-training easier to do on TPUs.  Post-training algorithms include continued pretraining, fine-tuning, PEFT, RLHF, and quantization. These algorithms adapt a pre-trained language model to a specific task or domain by training it on a custom dataset. Using TPUs provides significant advantages in terms of performance, cost-effectiveness, and scalability, enabling faster training times and the ability to work with larger models and datasets.

.. grid:: 1
    :gutter: 3
    :margin: 0
    :class-container: full-width g-0

    .. grid-item-card:: Choose Kithara for the following benefits:
        :padding: 1
        :class-item: g-0

        → Hugging Face checkpoints accepted and generated by default

        → Simple pip install

        → User-friendly documentation

        → Library-based UX

        → Wide variety of models through Keras Hub

        → Performant models through Maxtext

        → Custom trainer code in Keras or Jax

        → Ray data loading

        → Version Control

        → Reuse code on GPUs

        → Wide variety of tuning algorithms:  SFT, LoRA, Instruction tuning.

        → Coming soon: RL (DPO, PPO, ORPO, GRPO), Distillation, Quantization

        → Coming soon: Ray Tune for automatic hyperparameter selection


Target Audience
~~~~~~~~~~~~~~~

This guide is intended for developers and researchers familiar with machine learning and large language models who want to leverage the power of TPUs for fine-tuning and other post-training workloads. It is structured to guide you through the process, from setup to deployment, with practical examples and explanations of key concepts.

Key Features
~~~~~~~~~~~~

This product supports a variety of tuning algorithms (SFT, DPO, LoRA, Continued pre-training), data formats (JSONL, Parquet, CSV, Text, HuggingFace Dataset, and more), and models (including Gemma 2 2B, 9B, 27B; Llama 3.1 8B, 70B). It also includes performance optimizations (FlashAttention, Scanning, Rematerialization) and parallelism options (FSDP, FSDP+DDP).

Design Principles
~~~~~~~~~~~~~~~~~~
Easy to use
    → Frictionless onboarding

Hardware Flexibility
    → Use the same code on GPU and TPU

Developer Centric
    → Designed for customization instead of black-box recipes


Get Started
-----------
.. grid:: 2
    :gutter: 3
    :margin: 0
    :class-container: full-width g-0

    .. grid-item-card:: 🛒 Getting TPUs
        :link: getting_tpus
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `New to TPUs? Here is a guide for determining which TPUs to get and how to get them.`

    .. grid-item-card:: ⚒️ Installation
        :link: installation
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Quick PiP installation guide.`

    .. grid-item-card:: ✏️ Quickstart
        :link: quickstart
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Fine-tune a Gemma2 2B model using LoRA.`

    .. grid-item-card:: 📍 Finetuning Guide
        :link: finetuning_guide
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Step-by-step guide for finetuning your model.`

    .. grid-item-card:: 📈 Scaling up with Ray
        :link: scaling_with_ray
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `Guide for running multihost training with Ray.`

    .. grid-item-card:: 📖 API documentation
        :link: model_api
        :link-type: ref
        :columns: 4
        :padding: 2
        :class-item: g-0

        `API documentation for Kithara library components.`

.. toctree::
   :caption: Welcome
   :hidden:


   🛒 Getting TPUs <getting_tpus>
   ⚒️ Installation <installation>
   ✏️ Quickstart <quickstart>
   📎 Supported Models <models>
   📖 Supported Data Formats <datasets>
   🎯 Supported Optimizers <optimizers>
   📍 Finetuning Guide <finetuning_guide>
   📈 Scaling up with Ray <scaling_with_ray>
   🏁 Serve with vLLM <serve_with_vllm>
   💡 Troubleshooting <troubleshooting>
   💌 Help and Feedback <help_and_feedback>

.. toctree::
   :caption: Basics
   :hidden:

   🌵 SFT Example <sft>
   🌵 Continued Pretraining Example <pretraining>
   ✨ LoRA <lora>
   📦 Dataset Packing <packing>
   📚 Managing Large Datasets <ddp>
   🔎 Observability <observability>
   🔖 Checkpointing <checkpointing>
   🚀 Performance Optimizations <optimizations>

.. toctree::
   :caption: API
   :hidden:

   Model <api/kithara.model_api>
   Dataset <api/kithara.dataset_api>
   Trainer <api/kithara.trainer_api>

